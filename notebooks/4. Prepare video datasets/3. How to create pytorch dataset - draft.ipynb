{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants loaded\n"
     ]
    }
   ],
   "source": [
    "%run /home/dbaciur/NTU/NTU/notes/Constants.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the very first attempt to train network which was later replaced by automated tests runner in Python part of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LABELS_MAP = {\n",
    "    1: 0,\n",
    "    7: 1,\n",
    "    8: 2,\n",
    "    13: 3,\n",
    "    14: 4,\n",
    "    15: 5,\n",
    "    16: 6,\n",
    "    22: 7,\n",
    "    23: 8\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event id</th>\n",
       "      <th>Video</th>\n",
       "      <th>Time</th>\n",
       "      <th>Frame name</th>\n",
       "      <th>Behaviors</th>\n",
       "      <th>Layout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>PH1011-PHYSICS_20150922</td>\n",
       "      <td>65.873</td>\n",
       "      <td>PH1011-PHYSICS_20150922__Time65.873sec</td>\n",
       "      <td>[999]</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Event id                    Video    Time  \\\n",
       "0       NaN  PH1011-PHYSICS_20150922  65.873   \n",
       "\n",
       "                               Frame name Behaviors  Layout  \n",
       "0  PH1011-PHYSICS_20150922__Time65.873sec     [999]     3.0  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load frames df\n",
    "frames_df = pd.read_csv(C.Frames.FRAMES_DF_DRAFT_PATH, converters=C.F_CONVERTERS)\n",
    "frames_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check example frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PH1011-PHYSICS_20150922__Time65.873sec'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_file = frames_df.iloc[0][C.F_FRAME_NAME]\n",
    "frame_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 720, 1440, 3])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = torch.load(f\"{C.Frames.FRAMES_SCREENSHOTS_PATH}/{frame_file}.pt\")\n",
    "frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check example views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Event id</th>\n",
       "      <th>Video</th>\n",
       "      <th>Frame name</th>\n",
       "      <th>View names</th>\n",
       "      <th>View behaviors</th>\n",
       "      <th>View layouts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PH1011-PHYSICS_20150922</td>\n",
       "      <td>PH1011-PHYSICS_20150922__Time65.873sec</td>\n",
       "      <td>[PH1011-PHYSICS_20150922__Time65.873sec_VLeft,...</td>\n",
       "      <td>[999]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Event id                    Video  \\\n",
       "0   0       NaN  PH1011-PHYSICS_20150922   \n",
       "\n",
       "                               Frame name  \\\n",
       "0  PH1011-PHYSICS_20150922__Time65.873sec   \n",
       "\n",
       "                                          View names View behaviors  \\\n",
       "0  [PH1011-PHYSICS_20150922__Time65.873sec_VLeft,...          [999]   \n",
       "\n",
       "  View layouts  \n",
       "0       [1, 1]  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load views df\n",
    "views_df = pd.read_csv(C.VIEWS_DF_PATH, converters=C.V_CONVERTERS)\n",
    "views_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PH1011-PHYSICS_20150922__Time65.873sec_VLeft',\n",
       " 'PH1011-PHYSICS_20150922__Time65.873sec_VRight']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_view_names =  views_df[views_df[C.F_FRAME_NAME] == frame_file]\n",
    "frame_view_names = frame_view_names[C.V_VIEWS]\n",
    "frame_view_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PH1011-PHYSICS_20150922__Time65.873sec_VLeft'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's analyze only 1 view\n",
    "frame_view_name = frame_view_names[0][0]\n",
    "frame_view_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alex_view = torch.load(f\"{C.ALEX_NET_PATH}/{frame_view_name}.pt\")[0]\n",
    "alex_view.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepAlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_alex_view = torch.load(f\"{C.DEEP_ALEX_NET_PATH}/{frame_view_name}.pt\")[0]\n",
    "deep_alex_view.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_view = torch.load(f\"{C.RES_NET_PATH}/{frame_view_name}.pt\")[0]\n",
    "resnet_view.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4096])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_view = torch.load(f\"{C.VGG_PATH}/{frame_view_name}.pt\")[0]\n",
    "vgg_view.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25088])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_vgg_view = torch.load(f\"{C.DEEP_VGG_PATH}/{frame_view_name}.pt\")[0]\n",
    "deep_vgg_view.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing up\n",
    "\n",
    "**Video**:\n",
    "<br>\n",
    "data/Processed/VID.mp4\n",
    "\n",
    "\n",
    "**Frames**:\n",
    "<br>\n",
    "[1, 720, 1440, 3] <- can be different\n",
    "<br>\n",
    "Frames/VID__Time{time}sec.pt\n",
    "\n",
    "\n",
    "**View**:\n",
    "* data/Views/{}/VID__Time{time}sec__VFull.pt\n",
    "\n",
    "or:\n",
    "* data/Views/{}/VID__Time{time}sec__VLeft.pt\n",
    "* data/Views/{}/VID__Time{time}sec__VRight.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sample():\n",
    "    def __init__(self, id, event_id, video, views, label):\n",
    "        self.id = id\n",
    "        self.event_id = event_id\n",
    "        self.video = video\n",
    "        self.views = views\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_dict_list_values(d):\n",
    "    values = set()\n",
    "    for dlist in d.values():\n",
    "        for item in dlist:\n",
    "            values.add(item)\n",
    "    return values\n",
    "\n",
    "def subtract_lists(list1, list2):\n",
    "    return [item for item in list1 if item not in list2]\n",
    "\n",
    "def lists_intersect(list1, list2):\n",
    "    for item1 in list1:\n",
    "        if item1 in list2:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_view_ids_that_have_relevant_behs(views_df, relevant_behs):\n",
    "    ids = []\n",
    "    for i, row in views_df.iterrows():\n",
    "        id = row[C.V_ID]\n",
    "        behs = row[C.V_BEH]\n",
    "        \n",
    "        do_lists_interset = lists_intersect(behs, relevant_behs)\n",
    "        if do_lists_interset:\n",
    "            ids.append(id)\n",
    "    return ids\n",
    "\n",
    "def create_samples(views_dir, beh_labels_dict):\n",
    "    # this will be the resul\n",
    "    samples = []\n",
    "    \n",
    "    # get all behaviors to consider\n",
    "    all_behs = aggregate_dict_list_values(beh_labels_dict)\n",
    "    \n",
    "    views_df = pd.read_csv(C.VIEWS_DF_PATH, converters=C.V_CONVERTERS)\n",
    "    \n",
    "    # filter out all views that does not have any relevant behaviors\n",
    "    ids_of_relevant_views = get_view_ids_that_have_relevant_behs(views_df, all_behs)\n",
    "    views_df= views_df[views_df[C.V_ID].isin(ids_of_relevant_views)]\n",
    "    \n",
    "    # for every label -> [behaviors] mapping\n",
    "    for label, label_behs in tqdm(beh_labels_dict.items()):\n",
    "        \n",
    "        # get all behaviors that are NOT dedicated for given label\n",
    "        other_behs = subtract_lists(all_behs, label_behs)\n",
    "\n",
    "        # for every view\n",
    "        for i, views_row in views_df.iterrows():\n",
    "\n",
    "            id = views_row[C.V_ID]\n",
    "            event_id = views_row[C.V_EVENT_ID]\n",
    "            video = views_row[C.V_VIDEO]\n",
    "            behs = views_row[C.V_BEH]\n",
    "\n",
    "            # leave only behs that we're interested in\n",
    "            behs = [beh for beh in behs if beh in all_behs]\n",
    "            \n",
    "            # if view doesn't have at least 1 label behavior we skip it\n",
    "            behs_for_label = [beh for beh in behs if beh in label_behs]\n",
    "            if len(behs) == 0:\n",
    "                continue\n",
    "            \n",
    "            # if view has even 1 other behavior we also skip it\n",
    "            has_other_beh = lists_intersect(behs, other_behs)\n",
    "            if has_other_beh:\n",
    "                continue\n",
    "\n",
    "            label = torch.tensor(label)\n",
    "\n",
    "            view_names = views_row[C.V_VIEWS]\n",
    "            view_name1 = view_names[0]\n",
    "            view_name2 = view_name1 if len(view_names) == 1 else view_names[1]\n",
    "\n",
    "            view1 = torch.load(f\"{views_dir}/{view_name1}.pt\")[0]\n",
    "            if view_name2 == view_name1:\n",
    "                view2 = view1\n",
    "            else:\n",
    "                view2 = torch.load(f\"{views_dir}/{view_name2}.pt\")[0]\n",
    "\n",
    "            views = torch.cat([view1, view2])\n",
    "\n",
    "            samples.append(Sample(id, event_id, video, views, label))\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplesDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, samples, allowed_ids=None):\n",
    "        super(SamplesDataset, self).__init__()\n",
    "        \n",
    "        self.index_to_id = {}\n",
    "        self.items_dict = {}\n",
    "        self.length = 0\n",
    "        \n",
    "        current_index = 0\n",
    "        \n",
    "        for index, sample in enumerate(samples):\n",
    "            if allowed_ids is None or sample.id in allowed_ids:\n",
    "                self.index_to_id[current_index] = sample.id\n",
    "                self.items_dict[sample.id] = sample\n",
    "                \n",
    "                current_index = current_index + 1\n",
    "                self.length = self.length + 1\n",
    "    \n",
    "    def get_all_ids(self):\n",
    "        return list(self.index_to_id.values())\n",
    "            \n",
    "    def get_by_id(self, id):\n",
    "        return self.items_dict[id]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.items_dict)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= self.length:\n",
    "            raise IndexError\n",
    "\n",
    "        id = self.index_to_id[idx]\n",
    "        return self.items_dict[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1178e3dc02b4f9b8d100d8b747597d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjastrze/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "samples = create_samples(C.ALEX_NET_PATH, {\n",
    "    0: [C.B_CHARTS_S],\n",
    "    1: [C.B_EMPTY, C.B_WEBSITE_S, C.B_FILMS_S, C.B_WRITING]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6802"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SamplesDataset(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6802"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "views, label = sd[0].views, sd[0].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4096])\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "print(views.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_distribution(labels):\n",
    "    count_dict = {}\n",
    "    for label in labels:\n",
    "        label = label.item()\n",
    "        if count_dict.get(label) is None:\n",
    "            count_dict[label] = 0\n",
    "        count_dict[label] = count_dict[label] + 1\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1001, 1: 5801}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels = [item.label for item in ds]\n",
    "labels_count = get_labels_distribution(all_labels)\n",
    "labels_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, test, validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6802"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_items_ids = list(train_set.get_all_ids())\n",
    "all_items_count = len(all_items_ids)\n",
    "all_items_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sets(items, percentages):\n",
    "    items = np.array(items)\n",
    "    numpy.random.shuffle(items)\n",
    "    \n",
    "    splits = []\n",
    "    \n",
    "    length = len(items)\n",
    "    current_index = 0\n",
    "    \n",
    "    for p in percentages:\n",
    "        last_index = current_index + int(p * length)\n",
    "        \n",
    "        split = items[current_index:last_index]\n",
    "        splits.append(split)\n",
    "        \n",
    "        current_index = last_index\n",
    "    \n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = split_into_sets(all_items_ids, [.5, .3, .2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3401\n",
      "2040\n",
      "1360\n",
      "6801\n"
     ]
    }
   ],
   "source": [
    "print(len(splits))\n",
    "length = 0\n",
    "for s in splits:\n",
    "    length = length + len(s)\n",
    "    print(len(s))\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SamplesDataset(samples, splits[0])\n",
    "test_ds = SamplesDataset(samples, splits[1])\n",
    "validation_ds = SamplesDataset(samples, splits[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3401\n",
      "2040\n",
      "1360\n"
     ]
    }
   ],
   "source": [
    "print(len([s for s in train_ds]))\n",
    "print(len([s for s in test_ds]))\n",
    "print(len([s for s in validation_ds]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseEncoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SiameseEncoder, self).__init__()\n",
    "        \n",
    "        # Layers\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.activation1 = torch.nn.LeakyReLU()\n",
    "        \n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation2 = torch.nn.LeakyReLU()\n",
    "        \n",
    "        self.linear3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation3 = torch.nn.LeakyReLU()\n",
    "                \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape = (time, number_of_views, input_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        X = self.activation1(self.linear1(X))\n",
    "        X = self.activation2(self.linear2(X))\n",
    "        X = self.activation3(self.linear3(X))\n",
    "        \n",
    "        return X      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        # Layers\n",
    "        self.linear1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.activation1 = torch.nn.LeakyReLU()\n",
    "        \n",
    "        self.linear2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation2 = torch.nn.LeakyReLU()\n",
    "         \n",
    "#         self.linear3 = torch.nn.Linear(hidden_size, output_size)\n",
    "#         self.activation3 = torch.nn.Sigmoid();\n",
    "\n",
    "        self.linear3 = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.activation3 = torch.nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape = (time, input_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        X = self.activation1(self.linear1(X))\n",
    "        X = self.activation2(self.linear2(X))\n",
    "#         X = self.activation3(self.linear3(X))\n",
    "        X = self.linear3(X)\n",
    "        \n",
    "#         return X\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, encoder_hidden_size, classifier_hidden_size, output_size):\n",
    "        '''\n",
    "        Init method.\n",
    "        '''\n",
    "        super(FeatureClassifier, self).__init__()\n",
    "        \n",
    "        # Siamese Network\n",
    "        self.encoder = SiameseEncoder(input_size, encoder_hidden_size, encoder_hidden_size)\n",
    "        \n",
    "        # Pooling both\n",
    "        self.mp = torch.nn.MaxPool1d(2)\n",
    "        \n",
    "        # Classifier layer\n",
    "        self.classifier = Classifier(encoder_hidden_size, classifier_hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X.shape = (batch, views, encoding_size)\n",
    "        '''\n",
    "        X = self.encoder(X).permute(0,2,1)\n",
    "        X = self.mp(X).squeeze(2)\n",
    "        X = self.classifier(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = FeatureClassifier(input_size=4096,\n",
    "                       encoder_hidden_size=2048,\n",
    "                       classifier_hidden_size=2048,\n",
    "                       output_size=9).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=200, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=200, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(test, batch_size=200, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(nn.parameters(), lr=0.001, weight_decay = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 2, 4096])\n",
      "tensor([2, 2, 2, 7, 2, 7, 2, 4, 2, 6, 2, 4, 7, 6, 3, 3, 0, 7, 2, 6, 2, 1, 4, 4,\n",
      "        1, 4, 4, 7, 4, 6, 4, 0, 2, 3, 2, 1, 2, 2, 4, 2, 2, 7, 2, 4, 2, 5, 2, 4,\n",
      "        2, 7, 2, 2, 4, 1, 2, 4, 1, 2, 2, 7, 1, 7, 2, 1, 2, 4, 2, 2, 1, 7, 1, 1,\n",
      "        7, 5, 6, 6, 3, 5, 7, 4, 4, 1, 2, 4, 4, 4, 4, 6, 2, 2, 4, 1, 6, 6, 5, 4,\n",
      "        4, 4, 2, 3, 6, 5, 8, 6, 2, 3, 5, 2, 1, 2, 2, 6, 4, 2, 7, 2, 2, 4, 5, 5,\n",
      "        6, 0, 4, 6, 4, 5, 4, 1, 2, 1, 2, 2, 2, 4, 7, 4, 2, 1, 4, 6, 2, 5, 7, 1,\n",
      "        4, 2, 6, 7, 6, 2, 4, 6, 4, 1, 5, 4, 4, 4, 4, 2, 0, 1, 6, 2, 2, 4, 6, 2,\n",
      "        2, 2, 6, 1, 3, 4, 2, 4, 1, 2, 2, 4, 2, 2, 5, 6, 7, 4, 4, 4, 5, 2, 0, 4,\n",
      "        0, 4, 6, 4, 1, 4, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([200])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0061,  0.0212, -0.0205,  ...,  0.0046,  0.0342, -0.0081],\n",
       "        [ 0.0018,  0.0164, -0.0186,  ..., -0.0084,  0.0404, -0.0104],\n",
       "        [ 0.0077,  0.0073, -0.0161,  ..., -0.0181,  0.0352, -0.0127],\n",
       "        ...,\n",
       "        [ 0.0016,  0.0069, -0.0226,  ..., -0.0007,  0.0318, -0.0153],\n",
       "        [ 0.0036,  0.0136, -0.0159,  ..., -0.0030,  0.0357, -0.0181],\n",
       "        [-0.0018,  0.0197, -0.0239,  ...,  0.0018,  0.0381, -0.0089]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = nn(X.to('cpu'))\n",
    "print(pred.shape)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1983, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = FeatureClassifier(input_size=4096,\n",
    "                       encoder_hidden_size=2048,\n",
    "                       classifier_hidden_size=2048,\n",
    "                       output_size=9).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EL: 50.7, VL: 51.6, Progress: 95.7%: 100%|██████████| 500/500 [03:49<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "train_epoch_loss_chart = []\n",
    "train_batch_loss_chart = []\n",
    "valid_epoch_loss_chart = []\n",
    "\n",
    "EPOCHS = 500\n",
    "pbar = tqdm(range(EPOCHS))\n",
    "\n",
    "last_epoch_loss = 0\n",
    "valid_loss = 0\n",
    "best_valid_loss = float('inf')\n",
    "best_model = copy.deepcopy(nn.state_dict())\n",
    "for epoch in pbar:\n",
    "    \n",
    "    # Reset epoch_loss\n",
    "    epoch_loss = 0 \n",
    "    counter = 0\n",
    "    for X, y in train_loader:\n",
    "        \n",
    "        # Cuda\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Zero gradient\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # Make prediction\n",
    "        pred = nn(X.to('cuda'))\n",
    "        \n",
    "        # Calculate loss\n",
    "        batch_loss = loss(pred, y.to('cuda'))\n",
    "        \n",
    "        # Backpropagation\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        # Opt step\n",
    "        opt.step()\n",
    "        \n",
    "        # Update epoch_loss\n",
    "        batch_loss_value = batch_loss.detach().item()\n",
    "        epoch_loss += batch_loss_value\n",
    "        \n",
    "        # Add batch loss\n",
    "        train_batch_loss_chart.append(batch_loss_value)\n",
    "        \n",
    "        # Print Losses\n",
    "        epoch_loss_result = round(len(train_loader)*epoch_loss/(counter+1),1)\n",
    "        valid_loss_result = round(valid_loss, 1)\n",
    "        progress = round(counter/len(train_loader)*100, 1)\n",
    "        pbar.set_description(f'EL: {epoch_loss_result}, VL: {valid_loss_result}, Progress: {progress}%')\n",
    "        \n",
    "        # Update counter\n",
    "        counter += 1\n",
    "        \n",
    "    \n",
    "    # Update last\n",
    "    last_epoch_loss = epoch_loss\n",
    "    \n",
    "    # Update chart\n",
    "    train_epoch_loss_chart.append(epoch_loss)\n",
    "    \n",
    "    # Validation Dataset\n",
    "    \n",
    "    # Valid data\n",
    "    valid_loss = 0\n",
    "    for X, y in valid_loader:\n",
    "        valid_loss += loss(nn(X.to('cuda')), y.to('cuda')).detach().item()\n",
    "    valid_loss *= (len(train)/len(valid))\n",
    "    \n",
    "    # Saving Best Result:\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_model = copy.deepcopy(nn.state_dict())\n",
    "        best_valid_loss = valid_loss\n",
    "    \n",
    "    # Valid chart\n",
    "    valid_epoch_loss_chart.append(valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of performance per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "true = []\n",
    "for X, y in train_loader:\n",
    "    pred.append(nn(X.to('cuda')).to('cpu'))\n",
    "    true.append(y)\n",
    "where_equal = torch.cat(pred).max(1)[1] == torch.cat(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 2048])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction shape is 200 (batch size) times the number of possible classes\n",
    "pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4486, 2048])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0124,  0.0083, -0.0140,  ...,  0.0112,  0.0233,  0.0323],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example prediction\n",
    "p = pred[0][0]\n",
    "print(p.shape)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.0464, 0.0499, 0.0526,  ..., 0.0474, 0.0554, 0.0549],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([1711, 1711,  598,  ..., 1711, 1711, 1711]))"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pred = torch.cat(pred).max(1)\n",
    "max_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 9])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.zeros(9,9)\n",
    "zeros.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_performance(nn, loader):\n",
    "    pred = []\n",
    "    true = []\n",
    "    for X, y in loader:\n",
    "        pred.append(nn(X.to('cuda')).to('cpu'))\n",
    "        true.append(y)\n",
    "    where_equal = torch.cat(pred).max(1)[1] == torch.cat(true)\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = where_equal.float().mean().item()\n",
    "    \n",
    "    # Recall \n",
    "    recall = [where_equal[torch.cat(true) == i].float().mean().item() for i in range(9)]\n",
    "    \n",
    "    # Balanced accuracy \n",
    "    bacc = sum(recall)/len(recall)\n",
    "    \n",
    "    # Precision \n",
    "    precision = [0 if math.isnan(f) else f for f in [where_equal[torch.cat(pred).max(1)[1] == i].float().mean().item() for i in range(9)]]\n",
    "    \n",
    "    # Matrix of returns\n",
    "    odp = torch.zeros(9,9)\n",
    "    predicted = torch.cat(pred).max(1)[1].to('cpu')\n",
    "    for i in range(len(predicted)):\n",
    "        odp[predicted[i], torch.cat(true)[i]] += 1\n",
    "    \n",
    "    return {\n",
    "        'acc': acc,\n",
    "        'bacc': bacc,\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'number_of_obs': len(predicted),\n",
    "        'matrix': odp\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Weights to Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.load_state_dict(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1711 is out of bounds for dimension 0 with size 9",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-505351f6e938>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmp_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmp_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-160-0d779af28909>\u001b[0m in \u001b[0;36mmeasure_performance\u001b[0;34m(nn, loader)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0modp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     return {\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1711 is out of bounds for dimension 0 with size 9"
     ]
    }
   ],
   "source": [
    "mp_train = measure_performance(nn, train_loader)\n",
    "print('Train done!')\n",
    "mp_test = measure_performance(nn, test_loader)\n",
    "print('Test done!')\n",
    "mp_valid = measure_performance(nn, valid_loader)\n",
    "print('Valid done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_data = ALEX_NET_FRAME_VIEWS_DIR.split('/')[2].split('Views')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fdb5c07c4e0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(valid_epoch_loss_chart, label='Validation Loss', c='r')\n",
    "plt.plot(train_epoch_loss_chart, label='Training Loss')\n",
    "plt.title('Validation & Train Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# plt.savefig(f'./Plots/{name_of_data}___Validation_Training_Loss', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mp_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-a93d54cedf71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training Set'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Validation Set'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test Set'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmp_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmp_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy for Train, Validation and Test Sets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mp_train' is not defined"
     ]
    }
   ],
   "source": [
    "plt.bar(x=['Training Set', 'Validation Set', 'Test Set'], height=[mp_train['acc'],mp_valid['acc'],mp_test['acc']])\n",
    "plt.title('Accuracy for Train, Validation and Test Sets')\n",
    "plt.ylim([0,1])\n",
    "plt.ylabel('Accuracy')\n",
    "plt.text(-.25, .5, round(mp_train['acc'],3), c='white', size=20)\n",
    "plt.text(-.25 + 1, .5, round(mp_valid['acc'],3), c='white', size=20)\n",
    "plt.text(-.25 + 2, .5, round(mp_test['acc'],3), c='white', size=20)\n",
    "plt.savefig(f'./Plots/{name_of_data}___ACC', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
